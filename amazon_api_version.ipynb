{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test .ipynb\n",
    "Use this notebook to test how well your bot scrapes websites, parses titles and links, and uses semtiment analysis to choose the right stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import boto3, json, random, re, requests, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hooray, you guys! The market finished up today. \n",
      "\n",
      "The S&P500 closed at: \n",
      "3,066.91\n",
      "(+0.97%)\n",
      "\n",
      "My analysis concludes that it's because you can have afternoon tea with these cute (but naughty) sheep in scotland - travel+leisure:\n",
      "https://www.travelandleisure.com/animals/tea-with-naughty-sheep-airbnb\n"
     ]
    }
   ],
   "source": [
    "# Setting up requests package user agent.\n",
    "\n",
    "\"\"\"\n",
    "Create list of links and titles separately\n",
    "\"\"\"\n",
    "link_list = []\n",
    "title_list = []\n",
    "\n",
    "user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36\"\n",
    "headers = {\"User-Agent\": user_agent}\n",
    "\n",
    "# Scraping from The Dodo\n",
    "dodo_url = \"https://www.thedodo.com/close-to-home\"\n",
    "\n",
    "response = requests.get(dodo_url, headers=headers)\n",
    "dodo_soup = BeautifulSoup(response.content, \"lxml\")\n",
    "\n",
    "\"\"\"\n",
    "We are grabbing 'low hanging fruit' stories on the day.\n",
    "This is done via the 'close to home' page on the site.\n",
    "\"\"\"\n",
    "\n",
    "dodo_list = dodo_soup.findAll(\n",
    "    \"a\", attrs={\"class\": \"double-column-listing__link u-block-link ga-trigger\"}\n",
    ")\n",
    "\n",
    "[link_list.append(i[\"href\"]) for i in dodo_list]\n",
    "\n",
    "for i in dodo_list:\n",
    "    title_list.append(i.find(\"h2\").text.strip())\n",
    "\n",
    "# Huffpost Scrape\n",
    "\n",
    " # Google Scrape\n",
    "time.sleep(0.5)\n",
    "\n",
    "try:\n",
    "    goog_url = \"https://news.google.com/rss/search?q={cute+animals}\"\n",
    "    response = requests.get(goog_url, headers=headers)\n",
    "\n",
    "    goog_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    goog_list = goog_soup.findAll(\"item\")\n",
    "\n",
    "    for title in goog_list:\n",
    "        title_list.append(title.find(\"title\").text.strip())\n",
    "\n",
    "    for link in goog_list:\n",
    "        link_list.append(re.findall(\"<link/>(.*?)<guid\", str(link))[0])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Buzzpaws Scrape\n",
    "\n",
    "url = \"http://www.buzzpaws.com/\"\n",
    "response = requests.get(url, headers=headers)\n",
    "buzz_soup = BeautifulSoup(response.content, \"lxml\")\n",
    "\n",
    "buzz_list = buzz_soup.findAll(\n",
    "    \"div\", attrs={\"class\": \"content-thumb content-list-thumb\"}\n",
    ")\n",
    "\n",
    "for link in buzz_list:\n",
    "    link_list.append(link.find(\"a\")[\"href\"])\n",
    "\n",
    "for title in buzz_list:\n",
    "    title_list.append(title.find(\"a\")[\"title\"])\n",
    "\n",
    "title_list\n",
    "\n",
    "# Analyze sentiment\n",
    "\n",
    "client = boto3.client(\"comprehend\")\n",
    "sentiment = []\n",
    "for sentence in title_list:\n",
    "    sentiment.append(\n",
    "        client.detect_sentiment(Text=sentence, LanguageCode=\"en\")[\"Sentiment\"]\n",
    "    )\n",
    "\n",
    "# Render titles into lower case for later publishing\n",
    "title_list = [x.lower() for x in title_list]\n",
    "\n",
    "# So now we have link_list, sentiment, and title_list in memory\n",
    "\n",
    "# Discover Whether the Market Closed Up or Down on the Day\n",
    "\n",
    "# Reset the user agent and headers\n",
    "user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36\"\n",
    "headers = {\"User-Agent\": user_agent}\n",
    "\n",
    "# We can grab a closing price from Yahoo Finance\n",
    "url = \"https://finance.yahoo.com/quote/^GSPC\"\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "price_soup = BeautifulSoup(response.content, \"lxml\")\n",
    "\n",
    "# Extract the string we want with price information\n",
    "price_text = price_soup.findAll(\"div\", attrs={\"class\": \"D(ib) Mend(20px)\"})[0].text\n",
    "price_string = price_text[price_text.find(\"(\") + 1 : price_text.find(\")\")]\n",
    "\n",
    "\"\"\"\n",
    "Remove the four-digit delta in price, either positive or negative, from the Yahoo Finance\n",
    "string.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def price_format(price_string, price_text):\n",
    "    price_1 = re.sub(\n",
    "        r\"\\+[0-9]{1,2}[^(]*\", \" \", price_text, count=1\n",
    "    )  # + change; count=1 means remove only first instance\n",
    "    price_2 = re.sub(r\"\\-[0-9]{1,2}[^(]*\", \" \", price_text, count=1)  # - change\n",
    "\n",
    "    if price_string[0] == \"+\":  # positive delta\n",
    "        return price_1\n",
    "    else:\n",
    "        return price_2\n",
    "\n",
    "\n",
    "price_text = price_format(price_string, price_text)\n",
    "\n",
    "\n",
    "def price_clean(price_text):\n",
    "    # Remove remaining text\n",
    "    price_text = re.sub(\"[A-z]\", \"\", price_text)\n",
    "    # Remove remaining artifacts\n",
    "    price_text = re.sub(\"[ :]\", \"\", price_text)\n",
    "    price_text = re.sub(\"\\)[0-9]{3}\", \")\", price_text)\n",
    "    # Format the text for Twitter printing\n",
    "    price_text = price_text.replace(\"(\", \"\\n(\").replace(\")\", \")\\n\\n\")\n",
    "    # for some reason the chained replace was not working so it required a second pass, such...\n",
    "    price_text = price_text.replace(\"..\", \"\")\n",
    "    return price_text\n",
    "\n",
    "\n",
    "price_text = price_clean(price_text)\n",
    "\n",
    "market_up = [\n",
    "    \"Hooray, you guys! The market finished up today. \",\n",
    "    \"Finance Duck is jazzed. Markets r up.\",\n",
    "    \"Capitalism won today. Markets finished higher.\",\n",
    "    \"Damn, son. Markets up.\",\n",
    "    \"\"\"\n",
    "            Adorable 4pm\n",
    "            A happy market high-fives\n",
    "            because of the ducks\n",
    "            \"\"\",\n",
    "    \"Tell your friends. Market is up. All is well.\",\n",
    "    \"If the markets aren't up, I'm a swan.\",\n",
    "    \"\"\"\n",
    "            gusting proudly, bulls\n",
    "            savor passionate nectars,\n",
    "            bears crying\"\"\",\n",
    "    \"yessir, markets are up.\",\n",
    "    \"I'm pumped. Markets are too.\",\n",
    "]\n",
    "\n",
    "market_down = [\n",
    "    \"Dang it. Markets wet the bed.\",\n",
    "    \"Whatever. I don't even care that the markets finished down.\",\n",
    "    \"Well I'll be a lune's uncle; the markets finished down.\",\n",
    "    \"Pfffft. Stupid markets. They finished (eider)down. Ohhhhhhhhh...\",\n",
    "    \"Nnnnggg, bahhhhh.\",\n",
    "    \"Snap. Markets down a bit.\",\n",
    "    \"\"\"\n",
    "            Dire evening\n",
    "            A dark, failing market descends\n",
    "            forget the duck.\"\"\",\n",
    "    \"Flapping heck. Market down.\",\n",
    "    \"My net worth is down. So is the market.\",\n",
    "]\n",
    "\n",
    "# Generate a Message Related to Where the Market Finished\n",
    "\n",
    "# First, we want to generate two options for contextual link sentences based on whether the first word of our article title is a verb or not.\n",
    "\n",
    "noun = \"My analysis concludes that it's because this \"\n",
    "verb = \"My analysis concludes that you should \"\n",
    "num = \"My analysis concludes that the root cause is numeric. Here are \"\n",
    "neither = \"My analysis concludes that it's because \"\n",
    "\n",
    "\n",
    "# Now we want to randomly select a duck message based on whether the market finished up or down.\n",
    "def duck_message(market_up, market_down, price_string):\n",
    "    up_message = random.choice(market_up)\n",
    "    down_message = random.choice(market_down)\n",
    "    if price_string[0] == \"+\":\n",
    "        return up_message\n",
    "    else:\n",
    "        return down_message\n",
    "\n",
    "\n",
    "duck_talk = duck_message(market_up, market_down, price_string)\n",
    "\n",
    "# Now we want to select a good article title and link to behave as the causal force in the market as identified by Finance Duck.\n",
    "\n",
    "\n",
    "def title_message(sentiment, title_list, price_string):\n",
    "    # grab the index numbers of the respective sentiments\n",
    "    pos_index = [i for i, x in enumerate(sentiment) if x == \"POSITIVE\"]\n",
    "    neg_index = [i for i, x in enumerate(sentiment) if x == \"NEGATIVE\"]\n",
    "    neut_index = [i for i, x in enumerate(sentiment) if x == \"NEUTRAL\"]\n",
    "    # if the market finished up select our random positive message; if there is no pos message, go with a neutral\n",
    "    if price_string[0] == \"+\":\n",
    "        if len(pos_index) > 0:\n",
    "            choice = random.choice(pos_index)\n",
    "            return choice, title_list[choice]\n",
    "        else:\n",
    "            choice = random.choice(neut_index)\n",
    "            return choice, title_list[choice]\n",
    "    # if it finished down but there are no negative sentiment stories today, use the neutral messages...\n",
    "    elif len(neg_index) == 0:\n",
    "        choice = random.choice(neut_index)\n",
    "        return choice, title_list[choice]\n",
    "    else:\n",
    "        choice = random.choice(neg_index)\n",
    "        return choice, title_list[choice]\n",
    "\n",
    "\n",
    "# otherwise stay on plan and use the negative sentiment title\n",
    "\n",
    "\n",
    "choice, title_result = title_message(sentiment, title_list, price_string)\n",
    "\n",
    "# Parse the title to see if the first word is a verb or noun\n",
    "first_word = client.detect_syntax(Text=title_result, LanguageCode=\"en\")[\"SyntaxTokens\"][0][\"PartOfSpeech\"][\"Tag\"]\n",
    "\n",
    "\n",
    "def link_phrase(first_word, verb, noun, neither):\n",
    "    if first_word == \"VERB\":\n",
    "        return verb\n",
    "    elif first_word == \"NOUN\":\n",
    "        return noun\n",
    "    elif first_word == \"NUM\":\n",
    "        return num\n",
    "    else:\n",
    "        return neither\n",
    "\n",
    "\n",
    "anchor = link_phrase(first_word, verb, noun, neither)\n",
    "\n",
    "\"\"\"\n",
    "Link extraction from data frame. \n",
    "\"\"\"\n",
    "link = link_list[choice]\n",
    "\n",
    "\n",
    "# Compile the final message\n",
    "\n",
    "tweet = (\n",
    "    duck_talk\n",
    "    + \"\\n\\n\"\n",
    "    + \"The S&P500 closed at: \\n\"\n",
    "    + price_text\n",
    "    + anchor\n",
    "    + title_result\n",
    "    + \":\\n\"\n",
    "    + link\n",
    ")\n",
    "\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "consumer_token = \"KPifY4hb2LIGRxN8XlFBSAv4A\"\n",
    "consumer_secret = \"43TiVgZ0NcM31K2Bpf634xnHfbD6CsVgunG3S3iM6Y03EKJVXV\"\n",
    "access_token = \"1142771823471468545-dzvFy7BFtzedUhQbukZqT9MVosf2JC\"\n",
    "access_secret = \"zohTzltTJOc4GBSAGqoV9uVGqInLZYISoFX9GqmC0VBVC\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_token, consumer_secret)\n",
    "\n",
    "# Authenticate to Twitter\n",
    "auth = tweepy.OAuthHandler(consumer_token, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Tweet\n",
    "api.update_status(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Google News as a Finance Duck Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing Google News RSS Feed As a Source\n",
    "\"\"\"\n",
    "link_list = []\n",
    "title_list = []\n",
    "\n",
    "user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36\"\n",
    "headers = {\"User-Agent\": user_agent}\n",
    "\n",
    "# Scraping from The Dodo\n",
    "goog_url = \"https://news.google.com/rss/search?q={funny+animals}\"\n",
    "\n",
    "response = requests.get(goog_url, headers=headers)\n",
    "goog_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "\n",
    "goog_list = goog_soup.findAll(\"item\")\n",
    "\n",
    "for title in goog_list:\n",
    "    title_list.append(title.find(\"title\").text.strip())\n",
    "    \n",
    "for link in goog_list:\n",
    "    link_list.append(re.findall(\"<link/>(.*?)<guid\", str(link)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
